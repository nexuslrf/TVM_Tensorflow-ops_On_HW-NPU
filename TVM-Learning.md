# TVM Learning for Huawei Ascend

Mu Li's tutorial: http://tvm.d2l.ai.s3-website-us-west-2.amazonaws.com/

##Define tensor computation graph

* placeholder, compute
* var: to define shapes
* reduce_axis: for reduction operation
  * comm_reducer: for user defined reduce function
* if_then_else:  conditional expression
  * all & any: for complex conditional expression
* create_schedule: generate IR

## TVM Scheduling

### Scheduling on CPU

#### E.g. Matrix Multiplication

```python
def tvm_mm(n):
    A, B, C = d2ltvm.matmul(n, n, n)
    s = tvm.create_schedule(C.op)
    # Create a write cache for C
    CachedC = s.cache_write(C, 'local')
    # Same as before, first tile by blocks, and then parallelize the
    # computation of each block
    xo, yo, xi, yi = s[C].tile(*C.op.axis, tx, ty)
    xy = s[C].fuse(xo, yo)
    s[C].parallel(xy)
    # Use the write cache for the output of the xy axis, namely a block.
    s[CachedC].compute_at(s[C], xy)
    # Same as before to optimze the computation of a block .
    xc, yc = s[CachedC].op.axis
    ko, ki = s[CachedC].split(CachedC.op.reduce_axis[0], factor=tk)
    s[CachedC].reorder(ko, xc, ki, yc)
    s[CachedC].unroll(ki)
    s[CachedC].vectorize(yc)
    return s, (A, B, C)
```

### Scheduling on GPU

Official tutorial: https://docs.tvm.ai/tutorials/language/schedule_primitives.html

#### E.g. Vector Addition

```python
nt = 64  # number of threads in a block

def parallel(n):
    A, B, C = d2ltvm.vector_add(n)
    s = tvm.create_schedule(C.op)
    bx, tx = s[C].split(C.op.axis[0], factor=nt)
    s[C].bind(bx, tvm.thread_axis("blockIdx.x"))
    s[C].bind(tx, tvm.thread_axis("threadIdx.x"))
    return s, (A, B, C)
```

#### E.g. Matrix Multiplication

cache_read can create a read-only cache for  ğ´  that will be used by  ğ¶  on the shared memory, i.e. `s.cache_read(A, "shared", [C])`

```python
# Save into the d2ltvm package.
def split(stage, axis, factors):
    """Split an axis by a list of factors in a reverse order
    """
    axes = []
    for f in reversed(factors):
        axis, x = stage.split(axis, f)
        axes.append(x)
    return list(reversed(axes+[axis]))

# Save into the d2ltvm package.
def bind_thread(stage, axes, tags):
    """Bind a list of axes to thread axes
    """
    for axis, tag in zip(axes, tags):
        stage.bind(axis, tvm.thread_axis(tag))
```

```python
block_size = 16  # the number of threads for one dimension in a thread block.
tx, ty, tk = 8, 4, 32  # tile sizes for one CUDA thread
```

```python
def matmul_gpu(n):
    A, B, C = d2ltvm.matmul(n, n, n)
    s = tvm.create_schedule(C.op)
    # Create caches
    A_shared = s.cache_read(A, "shared", [C])
    A_local  = s.cache_read(A_shared, "local", [C])
    B_shared = s.cache_read(B, "shared", [C])
    B_local  = s.cache_read(B_shared, "local", [C])
    C_local = s.cache_write(C, "local")
    # Split each axis into block axis, thread axis, and inner axis
    x, y = s[C].op.axis
    xb, xo, xi = split(s[C], x, (block_size, tx))
    yb, yo, yi = split(s[C], y, (block_size, ty))
    s[C].reorder(xb, yb, xo, yo, xi, yi)
    # Note that we bind yb to blockIdx.x instead of blockIdx.y
    bind_thread(s[C], (yb, xb, yo, xo),
                ("blockIdx.x", "blockIdx.y", "threadIdx.x", "threadIdx.y"))
    # Schedule C_local
    s[C_local].compute_at(s[C], yo)
    yi, xi = s[C_local].op.axis
    k, = s[C_local].op.reduce_axis
    ko, ki = s[C_local].split(k, tk)
    s[C_local].reorder(ko, ki, yi, xi)
    # Optimize read caches of A and B with cooperative fetching
    def optimize_read_cache(shared, local):
        s[shared].compute_at(s[C_local], ko)
        s[local].compute_at(s[C_local], ki)
        y, x = s[shared].op.axis
        # Note that we must split into block_size parts to reuse
        # the previous axis threads
        yo, yi = s[shared].split(y, nparts=block_size)
        xo, xi = s[shared].split(x, nparts=block_size)
        s[shared].reorder(yo, xo, yi, xi)
        bind_thread(s[shared], (yo, xo), ("threadIdx.y", "threadIdx.x"))
    optimize_read_cache(A_shared, A_local)
    optimize_read_cache(B_shared, B_local)
    return s, (A, B, C)
```

### Scheduling on Ascend NPU

![](fig/Davinci.png)

NPU Info: dtype = float16, UB = 256KB, data per pass = 64KB

* cache_readä¸cache_writeéœ€è¦æˆå¯¹å‡ºç°ï¼ŒåŠŸèƒ½æ˜¯å¯¹tensoræ‰€åœ¨çš„bufferè¿›è¡Œæ˜ å°„ã€‚ç”±äºåœ¨TVMä¸­ï¼Œç”Ÿæˆçš„tensoré»˜è®¤åœ¨DDRä¸­ï¼Œä½†å¯¹äºæ˜‡è…¾AIå¤„ç†å™¨ï¼Œè®¡ç®—åœ¨AI Coreçš„UBä¸­ï¼Œæ‰€ä»¥éœ€è¦å°†tensorä»DDRæ¬è¿åˆ°UBè¿›è¡Œè®¡ç®—ï¼Œå†å°†è®¡ç®—ç»“æœä»UBæ¬å›DDRã€‚

  * scope_cbuf: L1
  * scope_ca: L0A
  * scope_cb: L0B
  * scope_cc: L0C
  * scope_ubuf: UB
  * scope_reg: REGï¼Œæ˜¯TBEå®šä¹‰çš„ç‰¹æ®Šscopeï¼Œå¯¹åº”äºå±€éƒ¨å˜é‡ã€‚

  [*Question*] will data on DDR automatically move to UB for computation?

* allocate_at å°†ä¸€ä¸ªstageçš„æ“ä½œæ‰‹åŠ¨allocateåˆ°å¦ä¸€ä¸ªstageçš„æŸæ ¹è½´ä¸Šï¼Œallocate_atéœ€è¦é…åˆcompute_atä½¿ç”¨ï¼Œä¸»è¦ç›®çš„æ˜¯ä¸€æ¬¡å¼€è¾Ÿè¾ƒå¤§çš„Nbuffer

  ```python
  s[valueA_UB].allocate_at(s[out], out.op.axis[0], run_once_axes=[out.op.axis[0], out.op.axis[1]])
  ```

  IR without allocate_at:

  ```c
  ...
      for (i, 0, 16) {
        for (j, 0, 16) {
          // attr [compute(valueA.local.UB, 0x563ad6cb6930)] realize_scope = "local.UB"
          realize valueA.local.UB([i, ((i + 1) - i)], *****[j, ((j + 1) - j)]*****, [0, 16], [0, 16])
  ...
  ```

  IR with allocate_at:

  ```c
  ...
     for (i, 0, 16) {
     // attr [compute(valueA.local.UB, 0x5618fc5ae210)] realize_scope = "local.UB"
     //realizeä¸Šæè‡³allocated atæŒ‡å®šè½´
        realize valueA.local.UB([i, ((i + 1) - i)], *****[0, 16]*****, [0, 16], [0, 16]) {
          for (j, 0, 16) {
  ```

  å½“allocate_atä¸double bufferä¸€èµ·ä½¿ç”¨æ—¶ï¼Œdouble bufferå˜ä¸ºåœ¨allocate atæŒ‡å®šè½´å±•å¼€ã€‚

* buffer_tileåŠbuffer_alignéƒ½æ˜¯å¯¹rootè½´ï¼ˆå³computeå®šä¹‰çš„è½´ï¼‰è¿›è¡Œçš„æ“ä½œï¼ŒæŒ‡å®šå®ƒcompute_atä¹‹åæ¯æ¬¡å¾ªç¯çš„èŒƒå›´ã€‚

  * ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ª2ç»´çš„tensor Aï¼š

    s[A].buffer_tile((i0.var, 32), (0,16))

    è¡¨ç¤ºå¯¹äº2ä¸ªforå¾ªç¯çš„èŒƒå›´ï¼Œç¬¬ä¸€ä¸ªforå¾ªç¯çš„minæŒ‡å®šä¸ºi0.varï¼ŒextentæŒ‡å®šä¸º32ï¼›ç¬¬äºŒä¸ªforå¾ªç¯çš„minæŒ‡å®šä¸º0ï¼Œextentä¸º16ã€‚

  * ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ª2ç»´çš„tensor B

    s[B].buffer_align((1, 32), (1, 16))

    è¡¨ç¤ºå¯¹äº2ä¸ªforå¾ªç¯çš„èŒƒå›´ï¼Œç¬¬ä¸€ä¸ªforå¾ªç¯çš„minå¯¹é½åˆ°1çš„å€æ•°ï¼Œextentå¯¹é½åˆ°32çš„å€æ•°ï¼›ç¬¬äºŒä¸ªforå¾ªç¯çš„èŒƒå›´minå¯¹é½åˆ°1çš„å€æ•°ï¼Œextentå¯¹é½åˆ°16çš„å€æ•°ã€‚

* reused_by(tensors), mem_unique()

  * ä¾‹å¦‚sch[A].reused_by(B, C, D)ï¼ŒæŒ‡å°†Aã€Bã€C ã€Då½“åšä¸€ä¸ªæ•´ä½“æ¥åˆ†é…å†…å­˜ï¼Œtensor Bã€Cã€Dç›´æ¥å¤ç”¨tensor Açš„bufferï¼Œå…¶ä¸­ï¼ˆB,C,Dï¼‰çš„ä¸ªæ•°å¯ä»¥æ˜¯>=1çš„ä»»æ„æ•°ç›®ã€‚
  * ä¾‹å¦‚ï¼šsch[E].mem_unique() ï¼Œè¡¨ç¤ºbuffer Eå†…å­˜å…¨å±€å”¯ä¸€ï¼Œä¸å’Œå…¶ä»–bufferåšå†…å­˜å¤ç”¨

* **double_buffer**: ä¸»è¦åŠŸèƒ½æ˜¯å¼€å¯æŒ‡å®šbufferçš„**pingã€pong**æµæ°´çº¿ï¼Œå³é€šè¿‡åŒç¼“å­˜æ–¹å¼è¿›è¡Œå½“å‰è®¡ç®—ã€‚å¼€å¯double_bufferåï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆä¸¤ä¸ªUBç©ºé—´ï¼Œå°†æ•°æ®åˆ†åˆ«æ¬å…¥ä»¥å‡å°‘è¯»å†™å†²çªã€‚

  * sch[A_buffer].double_buffer()ï¼ŒåŠŸèƒ½æ˜¯å¼€å¯A_bufferåœ¨å½“å‰forå¾ªç¯å†…éƒ¨çš„pingã€pongæµæ°´ã€‚

    *Note*ï¼š å½“å‰ç‰ˆæœ¬åªæ”¯æŒåœ¨å½“å‰forå¾ªç¯å†…éƒ¨å¼€å¯doule_bufferï¼Œä¸æ”¯æŒåœ¨forå¾ªç¯å˜é‡ä¸º1çš„è½´ä¸‹å¼€å¯double_bufferã€‚

  double_bufferå°†æŒ‡å®šforå¾ªç¯è½´ä¸‹çš„å˜é‡å¼€å¯pingã€pongæµæ°´ï¼Œä½¿å¾—å¾ªç¯å˜é‡å‡åŠï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

  ```
  for (1, n) {
    A       //å¼€å¯double_buffer
    ...
    B
    C      //å¼€å¯double_buffer
  }
  ```

  åˆ†åˆ«å¼€å¯Aä¸Cçš„double_bufferåï¼Œç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

  ```
  for (1, n/2) {
    A0
    ...
    B
    C0
    A1
    ...
    B
    C1
  }
  ```

* preload åŠŸèƒ½æ˜¯å°†double_bufferä¸­çš„pingçš„å†…å­˜æåˆ°å½“å‰forå¾ªç¯ä¹‹å¤–ã€‚

  * s[AL1].preload() å°†å¸¦æœ‰preload()æ ‡ç­¾çš„éƒ¨åˆ†å¤–æï¼Œå¤–æéƒ¨åˆ†ç”¨double_bufferçš„pingå†…å­˜å—ï¼Œç›®å‰åªæ”¯æŒå¤–æä¸€ä¸ªè½´ï¼ˆå¤–æåˆ°å½“å‰forå¾ªç¯ä¹‹å¤–ï¼‰

* **bind:** æŠŠæŸæ ¹è½´å’Œçº¿ç¨‹è½´è¿›è¡Œç»‘å®šï¼Œç”¨äºå®ç°å¤šæ ¸è®¡ç®—ã€‚

  ```python
   shape = [2,1024]
   A = tvm.placeholder(shape, name="A", dtype=dtype)
   B = tvm.compute(shape, lambda i,j: A[i,j]*2, name="B")
   s = tvm.create_schedule(B.op)
   thread_block = tvm.thread_axis("blockIdx.x")
   s[B].bind(B.op.axis[0], thread_block)
  ```

  å†…å±‚å¾ªç¯ï¼ˆå³å†…å±‚è½´ï¼‰å¯ä»¥ç›´æ¥é€šè¿‡ä¸€ä¸ªæŒ‡ä»¤å®ç°ï¼Œå¤–å±‚å¾ªç¯ï¼ˆå³å¤–å±‚è½´ï¼‰å¯ä»¥ä½¿ç”¨å¤šæ ¸æ–¹å¼å®ç°ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚å®ç°æ–¹æ³•ä¸ºï¼šå°†Bçš„è½´0ä¸æ˜‡è…¾AIå¤„ç†å™¨çš„å¹¶è¡Œè®¡ç®—è½´ï¼ˆblockIdx.xï¼‰ç»‘å®šã€‚

  thread_block = [tvm.thread_axis](https://docs.tvm.ai/api/python/tvm.html?highlight=thread_axis#tvm.thread_axis)("blockIdx.x")ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„çº¿ç¨‹å—å˜é‡è¡¨ç¤ºå¹¶è¡Œè®¡ç®—çš„è½´ï¼Œthread_blockä¸ºçº¿ç¨‹å˜é‡çš„å˜é‡åï¼ŒblockIdx.xè¡¨ç¤ºå½“å‰çº¿ç¨‹å—çš„æ ‡è®°ï¼Œç”¨æˆ·è‡ªå®šä¹‰å³å¯ã€‚

  s[B].bind(B.op.axis[0], thread_block)ï¼šå°†Bçš„è½´0ä¸å¹¶è¡Œè®¡ç®—è½´è¿›è¡Œç»‘å®šã€‚

  å¯¹åº”çš„IRè¡¨ç¤ºå¦‚ä¸‹ï¼š

  ```c
  produce B {
    // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 2
    for (j, 0, 1024) {
      B[((blockIdx.x*1024) + j)] = (A[((blockIdx.x*1024) + j)]*2.000000h)
    }
  }
  ```

  å³åŸæ¥çš„å¤–å±‚è½´2ä½¿ç”¨thread_extent=2è¿›è¡Œå®ç°ï¼Œè¡¨ç¤ºä¸¤ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œã€‚

#### Intrinsics Mapping:

```python
s[vlog_tL].emit_insn(li, 'vector_ln') 
s[vmuls_tL].emit_insn(mi, 'vector_muls') 
s[vexp_tL].emit_insn(ei, 'vector_exp')

s[dataL].emit_insn(di, 'dma_copy') 
s[vexp_t].emit_insn(xi, 'dma_copy')
```

#### Scheduling Optimization

**Data Tiling**

**Multi-Block**

**Double Buffer**