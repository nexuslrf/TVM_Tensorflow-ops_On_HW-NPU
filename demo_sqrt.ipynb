{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the Ascend TVM on Jupyter, you need to set some environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ps. Using Mind Studio via X11 is like eating shi*t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTHONPATH'] = '/home/team1/.mindstudio/huawei/ddk/1.60.T33.0.B162/ddk/site-packages/te.egg:'+\\\n",
    "    '/home/team1/.mindstudio/huawei/ddk/1.60.T33.0.B162/ddk/site-packages/topi.egg:'+\\\n",
    "    '/home/team1/1.60/MindStudio-ubuntu/tools/tbe_tools/py_utils/'\n",
    "\n",
    "os.environ['LD_LIBRARY_PATH'] = '/home/team1/.mindstudio/huawei/ddk/1.60.T33.0.B162/ddk/lib/x86_64-linux-gcc5.4/'\n",
    "\n",
    "os.environ['PATH'] += ':/home/team1/.mindstudio/huawei/ddk/1.60.T33.0.B162/ddk/toolchains/ccec-linux/bin/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from te import tvm\n",
    "from te.platform import cce_params as cce\n",
    "from te.platform.cce_build import build_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sqrt module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [512, 1024]\n",
    "dtype = 'float16'\n",
    "power_num = tvm.const(0.5, dtype = dtype)\n",
    "data = tvm.placeholder(shape, name=\"data\", dtype=dtype)\n",
    "vlog_t = tvm.compute(shape, lambda *indice: tvm.log(data(*indice)), name = \"vlog_t\")\n",
    "vmuls_t = tvm.compute(shape, lambda *indice: vlog_t(*indice) * power_num, name = \"vmuls_t\")\n",
    "vexp_t = tvm.compute(shape, lambda *indice: tvm.exp(vmuls_t(*indice)), name = \"vexp_t\")\n",
    "s = tvm.create_schedule(vexp_t.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [0] pragma_ = \"device\"\n",
      "// attr [vlog_t] storage_scope = \"global\"\n",
      "allocate vlog_t[float16 * 512 * 1024]\n",
      "// attr [vmuls_t] storage_scope = \"global\"\n",
      "allocate vmuls_t[float16 * 512 * 1024]\n",
      "produce vlog_t {\n",
      "  for (i0, 0, 512) {\n",
      "    for (i1, 0, 1024) {\n",
      "      vlog_t[((i0*1024) + i1)] = log(data[((i0*1024) + i1)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce vmuls_t {\n",
      "  for (i0, 0, 512) {\n",
      "    for (i1, 0, 1024) {\n",
      "      vmuls_t[((i0*1024) + i1)] = (vlog_t[((i0*1024) + i1)]*0.500000h)\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce vexp_t {\n",
      "  for (i0, 0, 512) {\n",
      "    for (i1, 0, 1024) {\n",
      "      vexp_t[((i0*1024) + i1)] = exp(vmuls_t[((i0*1024) + i1)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print IR, option\n",
    "with build_config:\n",
    "    print(tvm.lower(s, [data, vexp_t], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Optimize the Computation Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_ubuf = cce.scope_ubuf\n",
    "# Caching\n",
    "dataL = s.cache_read(data, scope_ubuf, [vlog_t])\n",
    "vlog_tL = s.cache_write(vlog_t, scope_ubuf)\n",
    "vmuls_tL = s.cache_write(vmuls_t, scope_ubuf)\n",
    "vexp_tL = s.cache_write(vexp_t, scope_ubuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove computation redundance\n",
    "s[vlog_t].compute_inline()\n",
    "s[vmuls_t].compute_inline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data spliting & tiling\n",
    "factor = 32\n",
    "xo, xi = s[vexp_t].split(vexp_t.op.axis[0], factor)     #xo=16, xi=32\n",
    "eo, ei = s[vexp_tL].split(vexp_tL.op.axis[0], factor)   #eo=16, ei=32\n",
    "mo, mi = s[vmuls_tL].split(vmuls_tL.op.axis[0], factor) #mo=16. mi=32\n",
    "lo, li = s[vlog_tL].split(vlog_tL.op.axis[0], factor)   #lo=16, li=32\n",
    "do, di = s[dataL].split(dataL.op.axis[0], factor)       #do=16, di=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ops Fusion\n",
    "s[vexp_tL].compute_at(s[vexp_t], xo)     #将vexp_tL语句块聚合到vexp_t的最外层轴上\n",
    "s[vmuls_tL].compute_at(s[vexp_t], xo)    #将vmuls_tL语句块聚合到vexp_t的最外层轴上\n",
    "s[vlog_tL].compute_at(s[vexp_t], xo)     #将vlog_tL语句块聚合到vexp_t的最外层轴上\n",
    "s[dataL].compute_at(s[vexp_t], xo)       #将dataL语句块聚合到vexp_t的最外层轴上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction Mapping\n",
    "s[vlog_tL].emit_insn(li, 'vector_ln')        #替换的轴为li=32,为最外层轴，即vlog_tL的整个代码块进行替换\n",
    "s[vmuls_tL].emit_insn(mi, 'vector_muls')     #替换的轴为mi=32,为最外层轴，即vmuls_tL的整个代码块进行替换\n",
    "s[vexp_tL].emit_insn(ei, 'vector_exp')       #替换的轴为ei=32,为最外层轴，即vexp_tL的整个代码块进行替换\n",
    "s[dataL].emit_insn(di, 'dma_copy')           #替换的轴为di=32,为最外层轴，即dataL的整个代码块进行替换\n",
    "s[vexp_t].emit_insn(xi, 'dma_copy')          #替换的轴为xi=32,为最外层轴，即vexp_t的整个代码块进行替换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Optimized IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [0] pragma_ = \"device\"\n",
      "// attr [data.local.UB] storage_scope = \"local.UB\"\n",
      "allocate data.local.UB[float16 * 32 * 1024]\n",
      " custom_new { 0 }\n",
      " custom_delete { nop(<args>); }\n",
      "// attr [vlog_t.local.UB] storage_scope = \"local.UB\"\n",
      "allocate vlog_t.local.UB[float16 * 32 * 1024]\n",
      " custom_new { 65536 }\n",
      " custom_delete { nop(<args>); }\n",
      "// attr [vmuls_t.local.UB] storage_scope = \"local.UB\"\n",
      "allocate vmuls_t.local.UB[float16 * 32 * 1024]\n",
      " custom_new { 131072 }\n",
      " custom_delete { nop(<args>); }\n",
      "produce vexp_t {\n",
      "  // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "  cce.coproc_dep_push(6, 5, 0)\n",
      "  set_vector_mask((uint64)18446744073709551615, (uint64)18446744073709551615)\n",
      "  for (i0.outer, 0, 16) {\n",
      "    produce data.local.UB {\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 5\n",
      "      cce.coproc_dep_pop(6, 5, 0)\n",
      "      copy_gm_to_ubuf(tvm_address_of(data.local.UB[0]), tvm_address_of(data[(i0.outer*32768)]), 0, 1, 2048, 0, 0)\n",
      "      cce.coproc_dep_push(5, 2, 0)\n",
      "    }\n",
      "    produce vlog_t.local.UB {\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "      cce.coproc_dep_pop(5, 2, 0)\n",
      "      vln(tvm_address_of(vlog_t.local.UB[0]), tvm_address_of(data.local.UB[0]), (uint8)255, (uint16)1, (uint16)1, (uint8)8, (uint8)8)\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "      vln(tvm_address_of(vlog_t.local.UB[32640]), tvm_address_of(data.local.UB[32640]), (uint8)1, (uint16)1, (uint16)1, (uint8)8, (uint8)8)\n",
      "    }\n",
      "    produce vmuls_t.local.UB {\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "      cce.coproc_sync(2)\n",
      "      vmuls(tvm_address_of(vmuls_t.local.UB[0]), tvm_address_of(vlog_t.local.UB[0]), 0.500000h, (uint8)255, (uint16)1, (uint16)1, (uint8)8, (uint8)8)\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "      vmuls(tvm_address_of(vmuls_t.local.UB[32640]), tvm_address_of(vlog_t.local.UB[32640]), 0.500000h, (uint8)1, (uint16)1, (uint16)1, (uint8)8, (uint8)8)\n",
      "    }\n",
      "    produce vexp_t.local.UB {\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "      cce.coproc_sync(2)\n",
      "      vexp(tvm_address_of(data.local.UB[0]), tvm_address_of(vmuls_t.local.UB[0]), (uint8)255, (uint16)1, (uint16)1, (uint8)8, (uint8)8)\n",
      "      // attr [iter_var(cce, , cce)] coproc_scope = 2\n",
      "      vexp(tvm_address_of(data.local.UB[32640]), tvm_address_of(vmuls_t.local.UB[32640]), (uint8)1, (uint16)1, (uint16)1, (uint8)8, (uint8)8)\n",
      "      cce.coproc_dep_push(2, 6, 0)\n",
      "    }\n",
      "    // attr [iter_var(cce, , cce)] coproc_scope = 6\n",
      "    cce.coproc_dep_pop(2, 6, 0)\n",
      "    copy_ubuf_to_gm(tvm_address_of(vexp_t[(i0.outer*32768)]), tvm_address_of(data.local.UB[0]), 0, 1, 2048, 0, 0)\n",
      "    cce.coproc_dep_push(6, 5, 0)\n",
      "  }\n",
      "  cce.coproc_dep_pop(6, 5, 0)\n",
      "}\n",
      "cce.coproc_sync(7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with build_config:\n",
    "    print(tvm.lower(s, [data, vexp_t], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with build_config:\n",
    "    tvm.build(s, [data, vexp_t], \"cce\", name=\"sqrt_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBE算子编译之后，会在当前目录下的kernel_meta目录下生成如下内容：\n",
    "* 算子的二进制文件*.o，文件以tvm.build中的name参数命名。\n",
    "* 算子的信息描述文件*.json，文件以tvm.build中的name参数命名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt_demo.json  sqrt_demo.o\r\n"
     ]
    }
   ],
   "source": [
    "ls kernel_meta/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
